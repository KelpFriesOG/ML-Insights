{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset\n",
    "\n",
    "When we start to talk about ML, one of the two most frequently used datasets to illustrate the power of machine learning is the MNIST dataset (the other is the CIFAR dataset).\n",
    "\n",
    "The MNIST dataset is composed of image samples of handwritten digits (0-9) that each have a square like resolution of 28x28 pixels.\n",
    "\n",
    "- The low definition of the images contributes to making the image processing task for the solution much simpler.\n",
    "- The `torchvision` based flavor of the dataset comes with **70000** images.\n",
    "\n",
    "When we choose to use any dataset we need to understand the concept of **normalizing the data.**\n",
    "\n",
    "---\n",
    "\n",
    "### What is normalization?\n",
    "\n",
    "Normalization is a fundamental preprocessing step in machine learning, especially when dealing with neural networks. It helps improve training stability, speeds up convergence, enhances performance, mitigates gradient issues, and improves robustness to variations in the input data. By understanding and applying normalization techniques, you can build more effective and reliable machine learning models.\n",
    "\n",
    "- Suppose the data for colored pixels. Each pixel typically has 4 channels (red, green, blue, alpha)\n",
    "\n",
    "- - The three color channels typically have values from 0-255 (for 8 bit colors).\n",
    "- - The alpha channel is typically on a straightforward range from 0-1 which indicates transpareny of the pixel itself.\n",
    "\n",
    "**For the MNIST dataset each pixel only has one channel (since it is monochrome) which goes from black (0) to white (255).**\n",
    "\n",
    "When we divide this single channel's values by 255 we get a normalized scale from 0 to 1 which helps a lot when dealing with training for reasons which will be discussed later.\n",
    "\n",
    "In Python and PyTorch / Tensorflow more specifically the notion of **tensors** is also very crucial.\n",
    "\n",
    "---\n",
    "\n",
    "### What are tensors?\n",
    "\n",
    "A tensor is a datatype in both PyTorch and Tensorflow\n",
    "\n",
    "In TensorFlow 2.x, tensors are stored and manipulated using NumPy arrays under the hood. This means that tensors are essentially views or references to NumPy arrays within TensorFlow's internal data structures. However, you can use PyTorch tensors in TensorFlow code if needed.\n",
    "\n",
    "In PyTorch, tensors are stored and managed as part of the PyTorch library itself. They are not directly related to NumPy arrays, although they can be converted between them for interoperability.\n",
    "\n",
    "A tensor is simply a multi-dimensional array.\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "- In mathematics, a tensor is an algebraic object that describes a **multilinear relationship between sets of algebraic objects related to a vector space**. Tensors may map between different objects such as vectors, scalars, and even other tensors.\n",
    "\n",
    "- In its simplest form a tensor can be a multidimensional array of scalars.\n",
    "\n",
    "However keep in mind that a tensor could represent more than just a singular set of values.\n",
    "\n",
    "- Consider the example of a colored image:\n",
    "\n",
    "- - **Each pixel can be represented as an array of four channels (r, g, b, a)**\n",
    "\n",
    "- - **We can also represent the location of each pixel in the image as an array two values (x, y)**\n",
    "\n",
    "Ultimately **we could create a tensor as follows: `[x, y, [r, g, b, a]]` and each tensor would capture all the relevant image of any given pixel**!\n",
    "\n",
    "- For the case of MNIST data each pixel only has one channel, lets call it hue, then the tensor that represents each pixel would be: `[x, y, hue]`\n",
    "\n",
    "#### Creating Tensors\n",
    "\n",
    "```python\n",
    "\n",
    "# TensorFlow 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_tensor = tf.constant([1, 2, 3], dtype=tf.int32)\n",
    "print(tf_tensor.dtype)  # Output: <dtype: 'int32'>\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "pytorch_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "print(pytorch_tensor.dtype)  # Output: Int64Tensor\n",
    "\n",
    "```\n",
    "\n",
    "#### Converting to tensors\n",
    "\n",
    "```python\n",
    "\n",
    "# Convert TensorFlow tensor to PyTorch tensor\n",
    "pytorch_tensor = tf.convert_to_tensor([1, 2, 3], dtype=tf.int32)\n",
    "print(pytorch_tensor.dtype)  # Output: Int64Tensor\n",
    "\n",
    "# Convert PyTorch tensor to TensorFlow tensor\n",
    "tf_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "print(tf_tensor.dtype)  # Output: <dtype: 'int32'>\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between PyTorch and TensorFlow tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int32\n",
      "<dtype: 'int32'>\n",
      "TensorFlow Tensor: <dtype: 'int32'>\n",
      "PyTorch Tensor: torch.int32\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create a PyTorch tensor\n",
    "pytorch_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "\n",
    "# Convert the NumPy array to a TensorFlow tensor\n",
    "tf_tensor = tf.convert_to_tensor(pytorch_tensor.numpy())\n",
    "\n",
    "print(pytorch_tensor.dtype) # Output: Int64Tensor\n",
    "print(tf_tensor.dtype) # Output: <dtype: 'int32'>\n",
    "\n",
    "# Create a TensorFlow tensor\n",
    "tf_tensor = tf.constant([1, 2, 3], dtype=tf.int32)\n",
    "\n",
    "# Convert the TensorFlow tensor to a NumPy array\n",
    "numpy_array = tf_tensor.numpy()\n",
    "\n",
    "# Convert the NumPy array back to a PyTorch tensor\n",
    "pytorch_tensor = torch.tensor(numpy_array, dtype=torch.int32)\n",
    "\n",
    "print(\"TensorFlow Tensor:\", tf_tensor.dtype)      # Output: <dtype: 'int32'>\n",
    "print(\"PyTorch Tensor:\", pytorch_tensor.dtype)  # Output: Int64Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the MNIST dataset (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Turn the input data into tensors\n",
    "    transforms.Normalize((0.5, ), (0.5, )) # Normalize the data\n",
    "])\n",
    "\n",
    "# Create the train and test portions of the data under ./data and apply the transformations to both!\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the batch size for each training loop and use it to instantiate DataLoader objects that work hand in hand with the training loop.\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the basic linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# If anything here confuses you, please refer back or re-read the math_for_ml notebook!\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # The first fully connected layer will take in 28x28 = 784 pixels as a flattened layer and output 64 values\n",
    "        # This means that each of the 784 pixels will be connected to each of the 64 \"middle layer\" nodes (hence fc for fully connected)\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        # Then each of the 64 \"middle layer\" or hidden nodes will connect to each of the 10 output nodes. Why 10? Well the hope is that\n",
    "        # each of the ten outputs will capture activate most with a respective digit (0 - 9)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Applying relu to the outputs of the first fully connected layer pass\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # Perform a simple straight pass through the second layer\n",
    "        x = self.fc2(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlshit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
